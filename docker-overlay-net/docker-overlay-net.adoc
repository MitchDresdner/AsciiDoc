= Docker Overlay Network
Mitch Dresdner <MDresdner@Terathink.com>
:toc:                                             // Enable table of contents [left, right]
:toc-placement: preamble
:appversion: 1.0.0
// A link as attribute
:fedpkg: https://apps.fedoraproject.org/packages/asciidoc
// Example of other attributes
:imagesdir: ./img
:icons: font
// Default icon dir is images/icons, can override using :iconsdir: ./icons
:stylesdir: ./styles
:scriptsdir: ./js
// keywords added to html
:keywords: docker, mule, mmc, deploy, microservice, monitoring

// enable btn:
:experimental:

[abstract]
Creating an overlay network for standalone containers

{sp} +

[.preamble]
// Preamble goes here


image::under-construction.jpg[Containers in action,700]

== Summary

When we get started using Docker, the typical configuration is to create a standalone application on our desktop.

For the most part, it's usually not practical to run all your applications on a single machine and when it's not you'll need
an approach for distributing the applications across many machines. This is where a Docker Swarm comes in.

Docker Swarm provides capabilities for clustering, scalability, discovery and security to name a few. In this
article, we'll create a basic Swarm configuration and perform some experiments to illustrate discovery and connectivity.

The demo Swarm cluster will consist of a Swarm manager and a worker, for convenience it will be running in AWS.

{sp} +

[NOTE]
====
_If you feel you're in need of a refresher on Docker Swarm or configuring your AWS account see https://dzone.com/articles/fun-with-docker-swarm[Dzone article: Fun with Docker Swarm] here._
====


{sp} +

== Architecture

Our target Architecture will consist of deploying Mule applications from AnypointStudio into Docker containers running inside an AWS AMI image.

{sp} +

image::under-construction.jpg[Mule in AWS,540]

{sp} +

In the steps below you'll configure an AWS EC2 instance and configure the environment for Docker. With Docker installed you'll
create images for Mule and MMC, run the containers, associate the Mule runtime with the MMC and deploy a simple _Hello Mule_ application.

Docker containers are ephemeral by nature, which means that any changes to container state will be lost when a container is
stopped. This isn't desirable and a quick remedy will be to use Docker volumes to persist and changes we need between container
starts. The information which will be stateful is specified by the *VOLUME* tag in the Docker files below.

{sp} +

== Prerequisites

We assume you're somewhat familiar with Docker and have some familiarity setting up EC2 instances in AWS.

If you're not confident with AWS or would like a little refresher please review the following articles:

{sp} +

.Some refreshers before getting started
* https://dzone.com/articles/provision-a-free-aws-ec2-instance-in-5-minutes[Provision a free tier EC2 instance]
* https://docs.docker.com/install/#cloud[Configure Docker on your EC2 instance]

{sp} +

=== Caveats
[CAUTION]
--
Some AWS services will incur charges, be sure to stop and/or terminate any services you aren't using. Additionaly,
consider setting up https://aws.amazon.com/about-aws/whats-new/2012/05/10/announcing-aws-billing-alerts/[billing alerts]
to warn you of charges exceeding a threshold that may caus you concern.

--

== Configuration

Begin by creating (2) EC2 instances, free tier should be fine, and install Docker on each EC2 instance.
Refer to the https://docs.docker.com/install/#supported-platforms[Docker Supported platforms] section for
Docker installation guidance and instructions for your instance.

{sp}+
AWS ports to open to support Docker swarm and our port connection test:

.Open ports in AWS Mule SG
|===
|Type|Protocol |Port Range|Source|Description

|Custom TCP Rule
|TCP
|2377
|10.193.142.0/24
|Docker swarm management

|Custom TCP Rule
|TCP
|7946
|10.193.142.0/24
|Container network discovery

|Custom UDP Rule
|TCP
|4789
|10.193.142.0/24
|Container ingress network

|Custom TCP Rule
|TCP
|8083
|10.193.142.0/24
|Demo port for machine to machine communications

|===

{sp} +

For our examples we'll use the following ip addresses to represent Node 1 and Node2.

* Node 1: _10.193.142.248_
* Node 2: _10.193.142.246_

{sp} +

Before getting started lets take a look at the existing Docker networks.

{sp} +

.List Docker networks
[listing]
--
docker network ls
--

{sp} +

The output of the network list should look at least like the listing below if youâ€™ve never added a network or
initialized a swarm on this Docker daemon. Other networks may be shown as well.

{sp} +
.Results of Docker network listing
[literal]
NETWORK ID          NAME                DRIVER              SCOPE
fa977e47b9f3        bridge              bridge              local
705fc078c278        host                host                local
bd4caf6c1751        none                null                local

{sp} +
From Node 1 lets begin by initializing the swarm.


.Create the swarm master node
[listing]
--
docker swarm init --advertise-addr=10.193.142.248
--

{sp} +
You should get a response that looks like the one below, we'll use the token provided to join our other node to the swarm.
{sp} +

....
Swarm initialized: current node (v9c2un5lqf7iapnv96uobag00) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-5bbh9ksinfmajdqnsuef7y5ypbwj5d9jazt47urenz3ksuw9lk-227dtheygwbxt8dau8ul791a7 10.193.142.248:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.

....

{sp} +

On Node 1 join the swarm as the manager

.Node 1 becomes manager
[listing]
--
docker swarm join-token manager
--

{sp} +
Next, lets join the swarm from Node 2.

{sp} +

.Node 2 joins swarm
[listing]
--
docker swarm join --token SWMTKN-1-5bbh9ksinfmajdqnsuef7y5ypbwj5d9jazt47urenz3ksuw9lk-227dtheygwbxt8dau8ul791a7 10.193.142.248:2377
This node joined a swarm as a worker.
--

{sp} +

From Node 1 the swarm master we can now look at the connected nodes

.On Master, list all nodes
[listing]
--
docker node ls
--

.Results of listing nodes
....
ID                            HOSTNAME            STATUS    AVAILABILITY   ENGINE VERSION
2quenyegseco1w0e5n1qe58r3     ip-10-193-142-235   Ready     Active         18.03.1-ce
wrjk02g909c6fnuxlepmksuz4     ip-10-193-142-246   Ready     Active         18.03.1-ce

....

{sp} +

Also, notice that an Ingress network has been created, this provides an entry point for our swarm network.

.Results of Docker network listing
[literal]
NETWORK ID          NAME                DRIVER              SCOPE
fa977e47b9f3        bridge              bridge              local
705fc078c278        host                host                local
bd4caf6c1751        none                null                local
qrppfipdu098        ingress             overlay             swarm

{sp} +


{sp} +
Lets go ahead and create our Overlay network for standalone containers

Create overlay network from Node 1

.Overlay network creation on Node 1
[listing]
--
docker network create --driver=overlay --attachable my-overlay-net

docker network ls
--

{sp} +
*Results of Docker network listing*
[literal]
NETWORK ID          NAME                DRIVER              SCOPE
fa977e47b9f3        bridge              bridge              local
705fc078c278        host                host                local
bd4caf6c1751        none                null                local
qrppfipdu098        ingress             overlay             swarm
vn12jyorp1ey        my-overlay-net      overlay             swarm

{sp} +

Note the addition of our new overlay network to the swarm

{sp} +

Join the overlay network from Node 1

.Run our container, join the overlay net
[listing]
--
docker run -it --name alpine1 --network my-overlay-net alpine
--

{sp} +

Join the overlay network from Node 2, we'll open port _8083_ to test connectivity into our running container.

.Run our container, join the overlay net
[listing]
--
docker run -it --name alpine2 -p 8083:8083 --network my-overlay-net alpine
--

{sp} +

== Verify our overlay network connectivity

With our containers running we can test that we can discover our hosts using DNS configured by the swarm.

From Node 2 lets ping the Nod 1 container.

.Node 2 pings Node 1, listens on port 8083
[listing]
--
ip addr   # show our ip address
ping -c 2 alpine1

# create listener on 8083
nc -l -p 8083

--

{sp} +

From Node 1 lets ping the Node 2 container and connect to it's open listener on port _8083_.

.Node 1 pings Node 2, connect to Node 2 listener on port 8083
[listing]
--
ip addr   # show our ip address
ping -c 2 alpine2

# connect to alpine2 listener on 8083
nc alpine2 8083
Hello Alpine2
^C
--

== Cleanup
With our testing complete we can tear down the swarm configuration.

.Remove Node 2 swarm
[listing]
--
docker container stop alpine2
docker container rm alpine2

docker swarm leave
--

{sp} +

.Remove Node 1 swarm
[listing]
--
docker container stop alpine1
docker container rm alpine1

docker swarm leave --force
--

{sp} +


This concludes our brief examples with creating Docker Overlay Networks. With these fundamental building blocks
in place, you now have the essential pieces necessary for building larger, more complex Docker container interactions.

Be sure to remove any AWS assets you may have used in these examples so you don't incur any ongoing costs.

{sp} +

I hope you enjoyed reading this article as much as I have enjoyed writing it, i'm looking forward to your feedback!

{sp} +
{sp} +

About the Author:

https://www.linkedin.com/in/mitch-dresdner-785a46126/[Mitch Dresdner] is a Senior Mule Consultant at TerraThink
